{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter channel index to analyze:1\n"
     ]
    }
   ],
   "source": [
    "chan_index = int(input('Enter channel index to analyze:'))\n",
    "# print('Enter Open Ephys data directory path:')\n",
    "# source_path = raw_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "import OpenEphys\n",
    "from kaveh.toolbox import common_avg_ref, butter_bandpass_filter\n",
    "import Kwik\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = []\n",
    "for root, dirnames, filenames in os.walk(source_path):\n",
    "    for filename in filenames:\n",
    "        f_regex = re.compile(r\".*CH(\\d|\\d\\d)\\.continuous$\")\n",
    "        if f_regex.match(filename):\n",
    "            f_names = f_names + [os.path.join(root, filename)]\n",
    "f_names = np.array(f_names)\n",
    "chans = [int(f.split('.')[-2].split('_')[-1][2:]) for f in f_names]\n",
    "f_names = f_names[np.argsort(chans)] # now sorted by channel number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH1.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH2.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH3.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH4.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH5.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH6.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH7.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH8.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH9.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH10.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH11.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH12.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH13.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH14.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH15.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH16.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH17.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH18.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH19.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH20.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH21.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH22.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH23.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH24.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH25.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH26.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH27.continuous...\n",
      "Loading continuous data...\n",
      "Reading ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/100_CH28.continuous...\n",
      "Loading continuous data...\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chan_data = []\n",
    "for contact in range(7):\n",
    "    contact_reps = []\n",
    "    for i in range(contact*4, contact*4 + 4):\n",
    "        print('Reading {}...'.format(f_names[i]))\n",
    "        file_content = OpenEphys.load(f_names[i])\n",
    "        Fs = float(file_content['header']['sampleRate'])\n",
    "        signal_filtered = butter_bandpass_filter(file_content['data'], 200, 10000, Fs, order=2 )\n",
    "        contact_reps.append(signal_filtered)\n",
    "    contact_reps = np.array(contact_reps)\n",
    "    chan_data.append(np.mean(contact_reps, axis=0))\n",
    "    print('-----------------------------------------------')\n",
    "chan_data = np.array(chan_data)\n",
    "contact_reps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving filtered data to ../data/OpenEphys_data/2019-09-09/2019-09-09_17-17-19/channels_filtered.npy\n"
     ]
    }
   ],
   "source": [
    "common_avg_ref(chan_data)\n",
    "chan_data = scipy.signal.detrend(chan_data)\n",
    "\n",
    "chan_filtered_fname = os.path.join(source_path, 'channels_filtered.npy')\n",
    "print('Saving filtered data to {}'.format(chan_filtered_fname))\n",
    "np.save(chan_filtered_fname, chan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chan_filtered_fname = os.path.join(source_path, 'channels_filtered.npy')\n",
    "# chan_data_raw = np.load(chan_filtered_fname)\n",
    "# Fs = 30000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot channel voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot channel\n",
    "# plt.figure(figsize=(16,5))\n",
    "# plt.plot(chan_data[chan_index, :])\n",
    "# plt.title('Channel {}. Signal length = {}s'.format(chan_index, chan_data.shape[1]/Fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process...\n",
      "Using minibatch spike detection, batch size = 50s, using filter (derivative) is False\n",
      "Using spikes min for detection. Aligning them to their min. \n",
      "Spike detection time = 8.739030599594116\n",
      "Using all spikes for alignment (no overlap removal)\n",
      "Align spikes time = 0.050180673599243164\n",
      "Clustering complex spikes...\n",
      "Splitting detected spikes for CS clustering ...\n",
      "CS spike detection time = 1.2418863773345947\n",
      "Merging overlapping CS waveforms\n",
      "CS post process time = 0.0013968944549560547\n"
     ]
    }
   ],
   "source": [
    "from cssorter.spikesorter import ComplexSpikeSorter\n",
    "dt = 1.0/Fs\n",
    "css = ComplexSpikeSorter(chan_data[chan_index, :], dt)\n",
    "css.num_gmm_components = 3\n",
    "css.cs_num_gmm_components = 6\n",
    "css.pre_window = 0.0002\n",
    "css.post_window = 0.003\n",
    "css.run(use_filtered=False, remove_overlap=False, spike_detection_dir = 'min', align_spikes_to='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 12\n",
    "pre_time = 0.0005\n",
    "post_time = 0.005\n",
    "clusters, cluster_labels = css.cluster_detected_cs(num_clusters=num_clusters, pre_time=pre_time, post_time=post_time)\n",
    "ss_indices = css.get_ss_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "pre_index = int(np.round(pre_time/css.dt))\n",
    "post_index = int(np.round(post_time/css.dt))\n",
    "aligned_ss = np.array([css.voltage[i - pre_index : i + post_index] for i in ss_indices[1:-2]])\n",
    "mean_ss = np.mean(aligned_ss[random.sample(range(0, aligned_ss.shape[0]), css.cs_indices.size), ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print('Total run time = {} s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cluster means\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0,1,num_clusters))\n",
    "legend_labels = []\n",
    "for cn in np.arange(num_clusters):\n",
    "    legend_labels.append('c{}({}) '.format(cn, clusters[cn].shape[0]))\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "plt.figure(figsize=(8,5))\n",
    "clust_means = []\n",
    "paxes = []\n",
    "for cn in np.arange(num_clusters):\n",
    "    clust_means.append(np.mean(clusters[cn], axis=0))\n",
    "    ax = plt.plot(clust_means[-1], color = colors[cn], label = legend_labels[cn])\n",
    "    paxes.append(ax)\n",
    "plt.plot(mean_ss, '--', label = 'Mean SS({})'.format(aligned_ss.shape[0]))\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(clust_means)\n",
    "cs_clust = np.argmax(np.sum(np.abs(kmeans.cluster_centers_), axis=1))\n",
    "clusters_to_pick = np.where(kmeans.labels_ == cs_clust)[0]\n",
    "print('Detected CS clusters: {}'.format(clusters_to_pick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans = input(\"Enter CS clusters (comma separated; example: 5,3,1). Type a character to accept the detected cluster: \")\n",
    "ans_regex = re.compile(r'\\d+(?:,\\d+)?')\n",
    "if ans_regex.match(ans):\n",
    "    clusters_to_pick = [int(c) for c in ans.split(',')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected CS: (1,)\n"
     ]
    }
   ],
   "source": [
    "cs_indices_to_pick = []\n",
    "for cti in clusters_to_pick:\n",
    "    cs_indices_to_pick = np.union1d(cs_indices_to_pick, css.cs_indices[np.where(cluster_labels == cti)])\n",
    "cs_indices = cs_indices_to_pick.astype(np.int64)\n",
    "print('Number of detected CS: {}'.format(cs_indices.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot channel\n",
    "f, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]}, figsize=(20,8), sharex=True)\n",
    "\n",
    "# prange = [css.signal_size*css.dt - 100, css.signal_size*css.dt] #s\n",
    "prange = [0,chan_data.shape[1]/Fs] #s\n",
    "idx_voltage = [int(pr*Fs) for pr in prange]\n",
    "ax0.plot(np.arange(prange[0],prange[1],1/Fs), chan_data[chan_index, idx_voltage[0]: idx_voltage[1]], alpha=0.5, color='k')\n",
    "# ax1.eventplot(css.cs_indices[np.where(np.logical_and(css.cs_indices < idx_voltage[1] , css.cs_indices >= idx_voltage[0]))]*css.dt, linelengths=50, lineoffsets=0, color='g')\n",
    "ax1.eventplot(cs_indices[np.where(np.logical_and(cs_indices < idx_voltage[1] , cs_indices >= idx_voltage[0]))]*css.dt, linelengths=50, lineoffsets=50, color='r')\n",
    "# ax1.eventplot(css.spike_indices[np.where(np.logical_and(css.spike_indices < idx_voltage[1] , css.spike_indices >= idx_voltage[0]))]*css.dt, linelengths=50, lineoffsets=100, color='m')\n",
    "# plt.plot(chan_data[chan_index,::10])\n",
    "plt.title('Channel {}. Signal length = {}s'.format(chan_index, chan_data.shape[1]/Fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 25\n",
    "\n",
    "SS = np.zeros(np.int64(css.signal_size/(Fs/1000.0)))\n",
    "SS[np.int64(ss_indices/(Fs/1000.0))] = 1\n",
    "CS = np.zeros(np.int64(css.signal_size/(Fs/1000.0)))\n",
    "CS[np.int64(cs_indices/(Fs/1000.0))] = 1\n",
    "\n",
    "\n",
    "\n",
    "xcor_vect = np.zeros((2*window + 1))\n",
    "first_cs_acor = np.argmax(cs_indices > window*(Fs/1000.0))\n",
    "last_cs_acor = np.argmax(cs_indices[::-1] < ss_indices[-1] - window*30) + 1\n",
    "for cti in np.int64(cs_indices[first_cs_acor:-1]/(Fs/1000.0)):\n",
    "    xcor_vect = xcor_vect + SS[cti-window : cti + window + 1]\n",
    "\n",
    "xcor_vect = xcor_vect / (cs_indices.size - (first_cs_acor + last_cs_acor))\n",
    "    \n",
    "acor_vect = np.zeros((2*window + 1))\n",
    "first_ss_acor = np.argmax(ss_indices > window*(Fs/1000.0))\n",
    "last_ss_acor = np.argmax(ss_indices[::-1] < ss_indices[-1] - window*(Fs/1000.0)) + 1\n",
    "for sti in np.int64(ss_indices[first_ss_acor:-last_ss_acor]/(Fs/1000.0)):\n",
    "    acor_vect = acor_vect + SS[sti-window : sti + window + 1]\n",
    "\n",
    "acor_vect[window] = 0\n",
    "\n",
    "acor_vect = acor_vect / (ss_indices.size - (first_ss_acor + last_ss_acor))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xcor_vect, 'r', alpha=0.35)\n",
    "plt.yticks(color='r')\n",
    "plt.fill_between(np.arange(0,2*window + 1), xcor_vect, color='r', alpha=0.25)\n",
    "plt.ylabel('P(SS(t) | CS(0)) Count', color = 'r')\n",
    "plt.xlabel('Time (ms)')\n",
    "\n",
    "plt.twinx()\n",
    "plt.plot(acor_vect, 'k', alpha=0.70)\n",
    "plt.yticks(color='k')\n",
    "plt.ylabel('P(SS(t) | SS(0)) Count', color = 'k')\n",
    "# plt.fill_between(acor_ret[0], 0, acor_ret[1] , color='g', alpha=0.25)\n",
    "plt.title('Conditional Probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1800000.0\n",
      "3600000.0\n",
      "5400000.0\n",
      "(176,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "num_clusters = 8\n",
    "pre_time = 0.0005\n",
    "post_time = 0.005\n",
    "align_to = 'min'\n",
    "pre_index = int(np.round(pre_time/css.dt))\n",
    "post_index = int(np.round(post_time/css.dt))\n",
    "delta = 60 # s\n",
    "cs_indices_all = []\n",
    "aligned_cs = []\n",
    "for ti in np.arange(0, css.signal_size, delta*Fs ):\n",
    "    print(ti)\n",
    "    if align_to is 'min':\n",
    "        aligned_cs = np.array([css.voltage[i - pre_index : i + post_index] for i in css.cs_indices if i < ti + delta*Fs and i > ti])\n",
    "    elif align_to is 'max':\n",
    "        aligned_cs = []\n",
    "        for csi in css.cs_indices[np.where(np.logical_and(cs_indices < ti + delta*Fs, cs_indices>ti))]:\n",
    "            align_point = np.argmax(css.voltage[csi - pre_index : csi + post_index]) + csi-pre_index\n",
    "            aligned_cs.append(css.voltage[align_point - pre_index : align_point + post_index])\n",
    "        aligned_cs = np.array(aligned_cs)\n",
    "    else:\n",
    "        raise ValueError(\"align_to should be min or max\")\n",
    "\n",
    "    gmm = GaussianMixture(num_clusters, covariance_type = 'full').fit(aligned_cs)\n",
    "    cluster_labels = gmm.predict(aligned_cs)\n",
    "    clusters = []\n",
    "\n",
    "    for cn in np.arange(num_clusters):\n",
    "        clusters.append(aligned_cs[np.where(cluster_labels == cn)])\n",
    "\n",
    "    clust_means = []\n",
    "    for cn in np.arange(num_clusters):\n",
    "        clust_means.append(np.mean(clusters[cn], axis=0))\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(clust_means)\n",
    "    cs_clust = np.argmax(np.sum(np.abs(kmeans.cluster_centers_), axis=1))\n",
    "    clusters_to_pick = np.where(kmeans.labels_ == cs_clust)[0]\n",
    "\n",
    "    cs_indices_to_pick = []\n",
    "    for cti in clusters_to_pick:\n",
    "        cs_indices_to_pick = np.union1d(cs_indices_to_pick, css.cs_indices[np.where(cluster_labels == cti)])\n",
    "    cs_indices_all.append(cs_indices_to_pick.astype(np.int64))\n",
    "cs_indices_all = np.hstack(cs_indices_all)\n",
    "print(cs_indices_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(cs_indices_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving detected CS and SS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_csv_filename = os.path.join(source_path, 'channel_{}.CS.csv'.format(chan_index))\n",
    "SS_csv_filename = os.path.join(source_path, 'channel_{}.SS.csv'.format(chan_index))\n",
    "\n",
    "import csv\n",
    "with open(CS_csv_filename, 'w+') as f:\n",
    "    print('writing {} ... '.format(CS_csv_filename))\n",
    "    f.seek(0)\n",
    "    csvwriter = csv.writer(f, delimiter = ',')\n",
    "    csvwriter.writerows(cs_indices.reshape(-1,1))\n",
    "    \n",
    "with open(SS_csv_filename, 'w+') as f:\n",
    "    print('writing {} ... '.format(SS_csv_filename))\n",
    "    f.seek(0)\n",
    "    csvwriter = csv.writer(f, delimiter = ',')\n",
    "    csvwriter.writerows(ss_indices.reshape(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
